{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import krippendorff\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from scipy.stats import f_oneway, tukey_hsd, entropy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "random.seed(1)"
   ],
   "id": "feae96c92353b786",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_path = os.path.join(\"Cleaned_AACEmoRepData_GPT4.xlsx\")\n",
    "dataset = pd.read_excel(dataset_path)\n",
    "\n",
    "words_data = dataset[dataset['representation_type'] == \"1\"]\n",
    "vad_data = dataset[dataset['representation_type'] == \"2\"]\n",
    "vadnum_data = dataset[dataset['representation_type'] == \"3\"]\n",
    "emojis_data = dataset[dataset['representation_type'] == \"4\"]\n",
    "\n",
    "response_representation_map = {\"1\": \"Words\", \"2\": \"VAD\", \"3\": \"Numeric_VAD\", \"4\": \"Emojis\"}\n",
    "emotions_map = {\n",
    "    1: [\"Grateful\", \"Very High Valence, Moderate Arousal, Low Dominance\", \"Valence: 2.5, Arousal: 0.0, Dominance: -2.5\", \"😊\"],\n",
    "    3: [\"Joyful\", \"Very High Valence, High Arousal, High Dominance\", \"Valence: 4.0, Arousal: 1.0, Dominance: 1.0\", \"😁\"],\n",
    "    5: [\"Content\", \"Very High Valence, High Arousal, Very High Dominance\", \"Valence: 4.0, Arousal: 0.0, Dominance: 4.0\", \"😌\"],\n",
    "    7: [\"Surprised\", \"High Valence, Very High Arousal, Moderate Dominance\", \"Valence: 1.0, Arousal: 2.5, Dominance: -2.5\", \"😯\"],\n",
    "    9: [\"Excited\", \"Very High Valence, Very High Arousal, High Dominance\", \"Valence: 2.5, Arousal: 4.0, Dominance: 1.0\", \"😃\"],\n",
    "    11: [\"Impressed\", \"High Valence, High Arousal, Low Dominance\", \"Valence: 1.0, Arousal: 1.0, Dominance: -4.0\", \"🙂\"],\n",
    "    13: [\"Proud\", \"Very High Valence, High Arousal, Very High Dominance\", \"Valence: 4.0, Arousal: 1.0, Dominance: 2.5\", \"🤩\"],\n",
    "    15: [\"Anxious\", \"Moderate Valence, High Arousal, Moderate Dominance\", \"Valence: -1.0, Arousal: 2.5, Dominance: -2.5\", \"😃\"],\n",
    "    17: [\"Afraid\", \"Very Low Valence, Very High Arousal, Low Dominance\", \"Valence: -5.0, Arousal: 2.5, Dominance: -4.0\", \"😨\"],\n",
    "    19: [\"Terrified\", \"Very Low Valence, Very High Arousal, Low Dominance\", \"Valence: -5.0, Arousal: 4.0, Dominance: -4.0\", \"😱\"],\n",
    "    21: [\"Annoyed\", \"Low Valence, Moderate Arousal, Moderate Dominance\", \"Valence: -2.5, Arousal: 0.0, Dominance: -1.0\", \"😒\"],\n",
    "    23: [\"Angry\", \"Low Valence, High Arousal, High Dominance\", \"Valence: -5.0, Arousal: 2.5, Dominance: 0.0\", \"😡\"],\n",
    "    25: [\"Furious\", \"Low Valence, Very High Arousal, High Dominance\", \"Valence: -4.0, Arousal: 4.0, Dominance: 1.0\", \"🤬\"],\n",
    "    27: [\"Sad\", \"Very Low Valence, Low Arousal, Low Dominance\", \"Valence: -4.0, Arousal: -2.5, Dominance: -4.0\", \"🙁\"],\n",
    "    29: [\"Devastated\", \"Very Low Valence, High Arousal, Low Dominance\", \"Valence: -5.0, Arousal: 1.0, Dominance: -2.5\", \"😭\"],\n",
    "    31: [\"Ashamed\", \"Low Valence, Mid Arousal, Low Dominance\", \"Valence: -3.0, Arousal: -1.0, Dominance: -4.0\", \"😳\"],\n",
    "    33: [\"Embarrassed\", \"Low Valence, High Arousal, Low Dominance\", \"Valence: -4.0, Arousal: 2.5, Dominance: -2.5\", \"😥\"],\n",
    "    35: [\"Guilty\", \"Low Valence, High Arousal, Low Dominance\", \"Valence: -4.0, Arousal: 0.0, Dominance: -4.0\", \"😬\"],\n",
    "    2: [\"Grateful\", \"Very High Valence,  Moderate Arousal, Low Dominance\", \"Valence: 2.5, Arousal: 0.0, Dominance: -2.5\", \"😊\"],\n",
    "    4: [\"Joyful\", \"Very High Valence,  High Arousal, High Dominance\", \"Valence: 4.0, Arousal: 1.0, Dominance: 1.0\", \"😁\"],\n",
    "    6: [\"Content\", \"Very High Valence, High Arousal, Very High Dominance\", \"Valence: 4.0, Arousal: 0.0, Dominance: 4.0\", \"😌\"],\n",
    "    8: [\"Surprised\", \"High Valence, Very High Arousal, Moderate Dominance\", \"Valence: 1.0, Arousal: 2.5, Dominance: -2.5\", \"😯\"],\n",
    "    10: [\"Excited\", \"Very High Valence, Very High Arousal, High Dominance\", \"Valence: 2.5, Arousal: 4.0, Dominance: 1.0\", \"😃\"],\n",
    "    12: [\"Impressed\", \"High Valence, High Arousal, Low Dominance\", \"Valence: 1.0, Arousal: 1.0, Dominance: -4.0\", \"🙂\"],\n",
    "    14: [\"Proud\", \"Very High Valence, High Arousal, Very High Dominance\", \"Valence: 4.0, Arousal: 1.0, Dominance: 2.5\", \"🤩\"],\n",
    "    16: [\"Anxious\", \"Moderate Valence, High Arousal, Moderate Dominance\", \"Valence: -1.0, Arousal: 2.5, Dominance: -2.5\", \"😃\"],\n",
    "    18: [\"Afraid\", \"Very Low Valence, Very High Arousal, Low Dominance\", \"Valence: -5.0, Arousal: 2.5, Dominance: -4.0\", \"😨\"],\n",
    "    20: [\"Terrified\", \"Very Low Valence, Very High Arousal, Low Dominance\", \"Valence: -5.0, Arousal: 4.0, Dominance: -4.0\", \"😱\"],\n",
    "    22: [\"Annoyed\", \"Low Valence, Moderate Arousal, Moderate Dominance\", \"Valence: -2.5, Arousal: 0.0, Dominance: -1.0\", \"😒\"],\n",
    "    24: [\"Angry\", \"Low Valence, High Arousal, High Dominance\", \"Valence: -5.0, Arousal: 2.5, Dominance: 0.0\", \"😡\"],\n",
    "    26: [\"Furious\", \"Low Valence, Very High Arousal, High Dominance\", \"Valence: -4.0, Arousal: 4.0, Dominance: 1.0\", \"🤬\"],\n",
    "    28: [\"Sad\", \"Very Low Valence, Low Arousal, Low Dominance\", \"Valence: -4.0, Arousal: -2.5, Dominance: -4.0\", \"🙁\"],\n",
    "    30: [\"Devastated\", \"Very Low Valence, High Arousal, Low Dominance\", \"Valence: -5.0, Arousal: 1.0, Dominance: -2.5\", \"😭\"],\n",
    "    32: [\"Ashamed\", \"Low Valence, Mid Arousal, Low Dominance\", \"Valence: -3.0, Arousal: -1.0, Dominance: -4.0\", \"😳\"],\n",
    "    34: [\"Embarrassed\", \"Low Valence, High Arousal, Low Dominance\", \"Valence: -4.0, Arousal: 2.5, Dominance: -2.5\", \"😥\"],\n",
    "    36: [\"Guilty\", \"Low Valence, High Arousal, Low Dominance\", \"Valence: -4.0, Arousal: 0.0, Dominance: -4.0\", \"😬\"],\n",
    "}\n"
   ],
   "id": "f984505dfc433ac3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset[\"Q573\"].dropna()",
   "id": "805e93972b6fe203",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Words",
   "id": "adb69e6aeffe6193"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "words_questions_col_names = [f\"Words_Question_{n}\" for n in range(1, 37)]\n",
    "words_questions_data = words_data[words_questions_col_names]\n",
    "\n",
    "word_data_cols = []\n",
    "\n",
    "for q_no, col in enumerate(words_questions_col_names):\n",
    "    words_data_qn = words_questions_data[words_questions_data[col].notna()]\n",
    "    current_data = words_data_qn[col].tolist()\n",
    "    while len(current_data) < 8:\n",
    "        current_data.append(\"\")\n",
    "    word_data_cols.append(current_data)\n",
    "    # print(f\"{len(words_data_qn)} people answered Words Question {q_no}. Each answer was picked this many times:\", words_data_qn[col].value_counts().to_dict())\n",
    "\n",
    "alpha = krippendorff.alpha(reliability_data=word_data_cols, level_of_measurement='nominal')\n",
    "print(\"Kripp's alpha\", alpha)"
   ],
   "id": "5d6cad5e11a26e43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Per User Likert",
   "id": "599a1c68169d75de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, we get all users who responded t",
   "id": "712ac30a5c0b8fef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "words_users_list = words_data[\"Q07\"].tolist()",
   "id": "fccd49b4840138e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "words_convey_scores, words_idsay_scores, words_elsesay_scores = [], [], []\n",
    "\n",
    "for user in words_users_list:\n",
    "    curr_user_data = words_data[words_data['Q07'] == user]\n",
    "    curr_user_data_dropped = curr_user_data.dropna(axis=1, how='any', inplace=True)    \n",
    "    convey_answers = [curr_user_data[col].tolist()[0] for col in curr_user_data.columns if 'Convey' in col]\n",
    "    idsay_answers = [curr_user_data[col].tolist()[0] for col in curr_user_data.columns if 'IdSay' in col]\n",
    "    elsesay_answers = [curr_user_data[col].tolist()[0] for col in curr_user_data.columns if 'ElseSay' in col] \n",
    "    convey_avg, idsay_avg, elsesay_avg = np.mean(convey_answers), np.mean(idsay_answers), np.mean(elsesay_answers)\n",
    "    words_convey_scores.append(convey_avg), words_idsay_scores.append(idsay_avg), words_elsesay_scores.append(elsesay_avg)\n",
    "    print(f\"For User: {user},\", \"Convey Score:\", convey_avg, \"I'd Say Score\", idsay_avg, \"Someone Else Say Score\", elsesay_avg)\n"
   ],
   "id": "fa6eb88943e5f81d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**For each choice question:**\n",
    "* Number of answers - 7 on average\n",
    "* Which answers and how many times - See table\n",
    "* User agreement (krippendorf's something?) - Full agreement on 11. Average shannon entropy of 0.32 across the whole set of questions.\n",
    "* Did answers converge more for later questions vs earlier questions? - They did not. Users did not become more confident as they answered more questions."
   ],
   "id": "8de702deb40fe6a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "w_q_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Answers_Picked\": [],\n",
    "    \"Answers_Normalized\": [],\n",
    "    \"Shannon_Entropy\": []\n",
    "}\n",
    "\n",
    "for q_no, col in enumerate(words_questions_col_names):\n",
    "    words_data_qn = words_questions_data[words_questions_data[col].notna()]\n",
    "    current_data = words_data_qn[col].tolist()\n",
    "\n",
    "    response_counts = words_data_qn[col].value_counts().to_dict()\n",
    "    proportions = {option: response_counts.get(option, 0)/sum(response_counts.values()) for option in [1, 2, 3, 4]}\n",
    "\n",
    "    w_q_data[\"Question_Num\"].append(q_no+1)\n",
    "    w_q_data[\"Num_Answers\"].append(len(words_data_qn))\n",
    "    w_q_data[\"Answers_Picked\"].append(response_counts)\n",
    "    w_q_data[\"Answers_Normalized\"].append(proportions)\n",
    "    w_q_data[\"Shannon_Entropy\"].append(entropy(list(proportions.values()), base=4))\n",
    "\n",
    "w_d_dataframe = pd.DataFrame(w_q_data)\n",
    "\n",
    "print(\n",
    "    \"Number of Answers for Choice Questions:\"\n",
    "    \"\\n\\tAverage:\", w_d_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMax:\", w_d_dataframe[\"Num_Answers\"].max(),\n",
    "    \"\\n\\tMin:\", w_d_dataframe[\"Num_Answers\"].min(),\n",
    "    \"\\n\\tMean Entropy:\", w_d_dataframe[\"Shannon_Entropy\"].mean()\n",
    ")\n",
    "\n",
    "w_d_dataframe"
   ],
   "id": "21ea8b2bfd4e8382",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_words_questions_answers = w_d_dataframe[\"Answers_Picked\"].apply(pd.Series).sum()\n",
    "\n",
    "all_words_questions_answers.index = all_words_questions_answers.index.astype(str).map(response_representation_map)\n",
    "all_words_questions_answers = all_words_questions_answers.reindex(list(response_representation_map.values()))\n",
    "\n",
    "plt.bar(all_words_questions_answers.index, all_words_questions_answers.values)\n",
    "plt.title(\"Words - Overall Answers Selected\")\n",
    "plt.show()\n"
   ],
   "id": "e50c2911993c080e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "w_convey_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Scores_Given\": [],\n",
    "    \"Mean_Score\": [],\n",
    "}\n",
    "\n",
    "words_convey_col_names = [col for col in words_data.columns if 'Words_Convey' in col]\n",
    "\n",
    "for q_no, col in enumerate(words_convey_col_names):\n",
    "    words_data_convey = words_data[words_data[col].notna()]\n",
    "    current_data = words_data_convey[col].tolist()\n",
    "\n",
    "    response_counts = words_data_convey[col].value_counts().to_dict()\n",
    "\n",
    "    w_convey_data[\"Question_Num\"].append(q_no+1)\n",
    "    w_convey_data[\"Num_Answers\"].append(len(words_data_convey))\n",
    "    w_convey_data[\"Scores_Given\"].append(response_counts)\n",
    "    w_convey_data[\"Mean_Score\"].append(sum([score*count for score, count in response_counts.items()])/len(words_data_convey))\n",
    "\n",
    "w_convey_dataframe = pd.DataFrame(w_convey_data)\n",
    "\n",
    "print(\n",
    "    \"Words - Scores for How well this Conveys:\"\n",
    "    \"\\n\\tAverage Number of Responses per question:\", w_convey_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMean Score:\", w_convey_dataframe[\"Mean_Score\"].mean(),\n",
    "    \"\\n\\tStandard Dev of Mean Scores:\", w_convey_dataframe[\"Mean_Score\"].std(),\n",
    "    \"\\n\\tVariance of Mean Score:\", w_convey_dataframe[\"Mean_Score\"].var()\n",
    ")\n",
    "\n",
    "w_convey_dataframe"
   ],
   "id": "f4910979c0bc8fb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_words_convey_scores = w_convey_dataframe[\"Scores_Given\"].apply(pd.Series).sum()\n",
    "\n",
    "all_words_convey_scores = all_words_convey_scores.reindex([1, 2, 3, 4, 5])\n",
    "\n",
    "all_words_convey_scores\n",
    "plt.bar(all_words_convey_scores.index, all_words_convey_scores.values)\n",
    "plt.title(\"Words - Total number of scores across all questions for Convey\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(w_convey_dataframe[\"Mean_Score\"], bins=10, range=(0, 5), edgecolor='black')\n",
    "plt.title('Words - Histogram of Mean Scores for Convey')\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "97902cfc5127b89a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "w_idsay_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Scores_Given\": [],\n",
    "    \"Mean_Score\": [],\n",
    "}\n",
    "\n",
    "words_idsay_col_names = [col for col in words_data.columns if 'Words_IdSay' in col]\n",
    "\n",
    "for q_no, col in enumerate(words_idsay_col_names):\n",
    "    words_data_idsay = words_data[words_data[col].notna()]\n",
    "    current_data = words_data_idsay[col].tolist()\n",
    "\n",
    "    response_counts = words_data_idsay[col].value_counts().to_dict()\n",
    "\n",
    "    w_idsay_data[\"Question_Num\"].append(q_no+1)\n",
    "    w_idsay_data[\"Num_Answers\"].append(len(words_data_idsay))\n",
    "    w_idsay_data[\"Scores_Given\"].append(response_counts)\n",
    "    w_idsay_data[\"Mean_Score\"].append(sum([score*count for score, count in response_counts.items()])/len(words_data_idsay))\n",
    "\n",
    "w_idsay_dataframe = pd.DataFrame(w_idsay_data)\n",
    "\n",
    "print(\n",
    "    \"Scores for How much I'd say this:\"\n",
    "    \"\\n\\tAverage Number of Responses per question:\", w_idsay_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMean Score:\", w_idsay_dataframe[\"Mean_Score\"].mean(),\n",
    "    \"\\n\\tStandard Dev of Mean Scores:\", w_idsay_dataframe[\"Mean_Score\"].std(),\n",
    "    \"\\n\\tVariance of Mean Score:\", w_idsay_dataframe[\"Mean_Score\"].var()\n",
    ")\n",
    "\n",
    "w_idsay_dataframe"
   ],
   "id": "a591d74c81d81d39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_words_idsay_scores = w_idsay_dataframe[\"Scores_Given\"].apply(pd.Series).sum()\n",
    "\n",
    "all_words_idsay_scores = all_words_idsay_scores.reindex([1, 2, 3, 4, 5])\n",
    "\n",
    "all_words_idsay_scores\n",
    "plt.bar(all_words_idsay_scores.index, all_words_idsay_scores.values)\n",
    "plt.title(\"Words - Total number of scores across all questions for I'd say.\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(w_idsay_dataframe[\"Mean_Score\"], bins=10, range=(0, 5), edgecolor='black')\n",
    "plt.title(\"Words - Histogram of Mean Scores for I'd say\")\n",
    "plt.show()"
   ],
   "id": "e34312db1781ccb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "w_elsesay_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Scores_Given\": [],\n",
    "    \"Mean_Score\": [],\n",
    "}\n",
    "\n",
    "words_elsesay_col_names = [col for col in words_data.columns if 'Words_ElseSay' in col]\n",
    "\n",
    "for q_no, col in enumerate(words_elsesay_col_names):\n",
    "    words_data_elsesay = words_data[words_data[col].notna()]\n",
    "    current_data = words_data_elsesay[col].tolist()\n",
    "\n",
    "    response_counts = words_data_elsesay[col].value_counts().to_dict()\n",
    "\n",
    "    w_elsesay_data[\"Question_Num\"].append(q_no+1)\n",
    "    w_elsesay_data[\"Num_Answers\"].append(len(words_data_elsesay))\n",
    "    w_elsesay_data[\"Scores_Given\"].append(response_counts)\n",
    "    w_elsesay_data[\"Mean_Score\"].append(sum([score*count for score, count in response_counts.items()])/len(words_data_elsesay))\n",
    "\n",
    "w_elsesay_dataframe = pd.DataFrame(w_elsesay_data)\n",
    "\n",
    "print(\n",
    "    \"Scores for How much I'd hear someone else say this:\"\n",
    "    \"\\n\\tAverage Number of Responses per question:\", w_elsesay_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMean Score:\", w_elsesay_dataframe[\"Mean_Score\"].mean(),\n",
    "    \"\\n\\tStandard Dev of Mean Scores:\", w_elsesay_dataframe[\"Mean_Score\"].std(),\n",
    "    \"\\n\\tVariance of Mean Score:\", w_elsesay_dataframe[\"Mean_Score\"].var()\n",
    ")\n",
    "\n",
    "w_elsesay_dataframe"
   ],
   "id": "a047528766870306",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_words_elsesay_scores = w_elsesay_dataframe[\"Scores_Given\"].apply(pd.Series).sum()\n",
    "\n",
    "all_words_elsesay_scores = all_words_elsesay_scores.reindex([1, 2, 3, 4, 5])\n",
    "\n",
    "all_words_elsesay_scores\n",
    "plt.bar(all_words_elsesay_scores.index, all_words_elsesay_scores.values)\n",
    "plt.title(\"Words - Total number of scores across all questions for Someone else'd say.\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(w_elsesay_dataframe[\"Mean_Score\"], bins=10, range=(0, 5), edgecolor='black')\n",
    "plt.title(\"Words - Histogram of Mean Scores for Someone else'd say\")\n",
    "plt.show()"
   ],
   "id": "879fcaefb918e2f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# VAD",
   "id": "2c6af9743adace0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vad_questions_col_names = [f\"VAD_Question_{n}\" for n in range(1, 37)]\n",
    "vad_questions_data = vad_data[vad_questions_col_names]\n",
    "\n",
    "vad_data_cols = []\n",
    "\n",
    "for q_no, col in enumerate(vad_questions_col_names):\n",
    "    vad_data_qn = vad_questions_data[vad_questions_data[col].notna()]\n",
    "    current_data = vad_data_qn[col].tolist()\n",
    "    while len(current_data) < 9:\n",
    "        current_data.append(\"\")\n",
    "    vad_data_cols.append(current_data)\n",
    "    # print(f\"{len(vad_data_qn)} people answered VAD Question {q_no}. Each answer was picked this many times:\", vad_data_qn[col].value_counts().to_dict())\n",
    "\n",
    "alpha = krippendorff.alpha(reliability_data=vad_data_cols, level_of_measurement='nominal')\n",
    "print(\"Kripp's alpha\", alpha)"
   ],
   "id": "df70590115ac0d11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Per User Likert",
   "id": "93d5e1b25eff776"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vad_users_list = vad_data[\"Q07\"].tolist()",
   "id": "4f1427b7fc01dd21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vad_convey_scores, vad_idsay_scores, vad_elsesay_scores = [], [], []\n",
    "\n",
    "for user in vad_users_list:\n",
    "    curr_user_data = vad_data[vad_data['Q07'] == user]    \n",
    "    curr_user_data_dropped = curr_user_data.dropna(axis=1, how='any', inplace=True)    \n",
    "    convey_answers = [curr_user_data[col].tolist()[0] for col in curr_user_data.columns if 'Convey' in col]\n",
    "    idsay_answers = [curr_user_data[col].tolist()[0] for col in curr_user_data.columns if 'IdSay' in col]\n",
    "    elsesay_answers = [curr_user_data[col].tolist()[0] for col in curr_user_data.columns if 'ElseSay' in col] \n",
    "    convey_avg, idsay_avg, elsesay_avg = np.mean(convey_answers), np.mean(idsay_answers), np.mean(elsesay_answers)\n",
    "    vad_convey_scores.append(convey_avg), vad_idsay_scores.append(idsay_avg), vad_elsesay_scores.append(elsesay_avg)\n",
    "    print(f\"For User: {user},\", \"Convey Score:\", convey_avg, \"I'd Say Score\", idsay_avg, \"Someone Else Say Score\", elsesay_avg)\n"
   ],
   "id": "4ba4dadc21860751",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**For each choice question:**\n",
    "* Number of answers - 7 on average\n",
    "* Which answers and how many times - See table\n",
    "* User agreement (krippendorf's something?) - Full agreement on none. Average shannon entropy of 0.61 across the whole set of questions.\n",
    "* Did answers converge more for later questions vs earlier questions? - They did not. Users did not become more confident as they answered more questions."
   ],
   "id": "9b74a116157365fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vad_q_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Answers_Picked\": [],\n",
    "    \"Answers_Normalized\": [],\n",
    "    \"Shannon_Entropy\": []\n",
    "}\n",
    "\n",
    "for q_no, col in enumerate(vad_questions_col_names):\n",
    "    vad_data_qn = vad_questions_data[vad_questions_data[col].notna()]\n",
    "    current_data = vad_data_qn[col].tolist()\n",
    "\n",
    "    response_counts = vad_data_qn[col].value_counts().to_dict()\n",
    "    proportions = {option: response_counts.get(option, 0)/sum(response_counts.values()) for option in [1, 2, 3, 4]}\n",
    "\n",
    "    vad_q_data[\"Question_Num\"].append(q_no+1)\n",
    "    vad_q_data[\"Num_Answers\"].append(len(vad_data_qn))\n",
    "    vad_q_data[\"Answers_Picked\"].append(response_counts)\n",
    "    vad_q_data[\"Answers_Normalized\"].append(proportions)\n",
    "    vad_q_data[\"Shannon_Entropy\"].append(entropy(list(proportions.values()), base=4))\n",
    "\n",
    "vad_d_dataframe = pd.DataFrame(vad_q_data)\n",
    "\n",
    "print(\n",
    "    \"Number of Answers for VAD Choice Questions:\"\n",
    "    \"\\n\\tAverage:\", vad_d_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMax:\", vad_d_dataframe[\"Num_Answers\"].max(),\n",
    "    \"\\n\\tMin:\", vad_d_dataframe[\"Num_Answers\"].min(),\n",
    "    \"\\n\\tMean Entropy:\", vad_d_dataframe[\"Shannon_Entropy\"].mean()\n",
    ")\n",
    "\n",
    "vad_d_dataframe"
   ],
   "id": "952ff52476d45d3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_vad_questions_answers = vad_d_dataframe[\"Answers_Picked\"].apply(pd.Series).sum()\n",
    "\n",
    "all_vad_questions_answers.index = all_vad_questions_answers.index.astype(str).map(response_representation_map)\n",
    "all_vad_questions_answers = all_vad_questions_answers.reindex(list(response_representation_map.values()))\n",
    "\n",
    "plt.bar(all_vad_questions_answers.index, all_vad_questions_answers.values)\n",
    "plt.title(\"VAD - Overall Answers selected\")\n",
    "plt.show()"
   ],
   "id": "e6f5376fd09686c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vad_convey_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Scores_Given\": [],\n",
    "    \"Mean_Score\": [],\n",
    "}\n",
    "\n",
    "vad_convey_col_names = [col for col in words_data.columns if 'VAD_Convey' in col]\n",
    "\n",
    "\n",
    "for q_no, col in enumerate(vad_convey_col_names):\n",
    "    vad_data_convey = vad_data[vad_data[col].notna()]\n",
    "    current_data = vad_data_convey[col].tolist()\n",
    "\n",
    "    response_counts = vad_data_convey[col].value_counts().to_dict()\n",
    "\n",
    "    vad_convey_data[\"Question_Num\"].append(q_no+1)\n",
    "    vad_convey_data[\"Num_Answers\"].append(len(vad_data_convey))\n",
    "    vad_convey_data[\"Scores_Given\"].append(response_counts)\n",
    "    if len(vad_data_convey) == 0: vad_convey_data[\"Mean_Score\"].append(0)\n",
    "    else: vad_convey_data[\"Mean_Score\"].append(sum([score*count for score, count in response_counts.items()])/len(vad_data_convey))\n",
    "\n",
    "vad_convey_dataframe = pd.DataFrame(vad_convey_data)\n",
    "\n",
    "print(\n",
    "    \"VAD - Scores for How well this Conveys:\"\n",
    "    \"\\n\\tAverage Number of Responses per question:\", vad_convey_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMean Score:\", vad_convey_dataframe[\"Mean_Score\"].mean(),\n",
    "    \"\\n\\tStandard Dev of Mean Scores:\", vad_convey_dataframe[\"Mean_Score\"].std(),\n",
    "    \"\\n\\tVariance of Mean Score:\", vad_convey_dataframe[\"Mean_Score\"].var()\n",
    ")\n",
    "\n",
    "vad_convey_dataframe"
   ],
   "id": "e7abab24405c0ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_vad_convey_scores = vad_convey_dataframe[\"Scores_Given\"].apply(pd.Series).sum()\n",
    "\n",
    "all_vad_convey_scores = all_vad_convey_scores.reindex([1, 2, 3, 4, 5])\n",
    "\n",
    "all_vad_convey_scores\n",
    "plt.bar(all_vad_convey_scores.index, all_vad_convey_scores.values)\n",
    "plt.title(\"VAD - Total number of scores across all questions for Convey\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(vad_convey_dataframe[\"Mean_Score\"], bins=10, range=(0, 5), edgecolor='black')\n",
    "plt.title('VAD - Histogram of Mean Scores for Convey')\n",
    "plt.show()"
   ],
   "id": "bb59cf6882c66d0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vad_idsay_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Scores_Given\": [],\n",
    "    \"Mean_Score\": [],\n",
    "}\n",
    "\n",
    "vad_idsay_col_names = [col for col in vad_data.columns if 'VAD_IdSay' in col]\n",
    "\n",
    "for q_no, col in enumerate(vad_idsay_col_names):\n",
    "    vad_data_idsay = vad_data[vad_data[col].notna()]\n",
    "    current_data = vad_data_idsay[col].tolist()\n",
    "\n",
    "    response_counts = vad_data_idsay[col].value_counts().to_dict()\n",
    "\n",
    "    vad_idsay_data[\"Question_Num\"].append(q_no+1)\n",
    "    vad_idsay_data[\"Num_Answers\"].append(len(vad_data_idsay))\n",
    "    vad_idsay_data[\"Scores_Given\"].append(response_counts)\n",
    "    if len(vad_data_idsay) == 0: vad_idsay_data[\"Mean_Score\"].append(0)\n",
    "    else: vad_idsay_data[\"Mean_Score\"].append(sum([score*count for score, count in response_counts.items()])/len(vad_data_idsay))\n",
    "\n",
    "vad_idsay_dataframe = pd.DataFrame(vad_idsay_data)\n",
    "\n",
    "print(\n",
    "    \"VAD - Scores for How much I'd say this:\"\n",
    "    \"\\n\\tAverage Number of Responses per question:\", vad_idsay_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMean Score:\", vad_idsay_dataframe[\"Mean_Score\"].mean(),\n",
    "    \"\\n\\tStandard Dev of Mean Scores:\", vad_idsay_dataframe[\"Mean_Score\"].std(),\n",
    "    \"\\n\\tVariance of Mean Score:\", vad_idsay_dataframe[\"Mean_Score\"].var()\n",
    ")\n",
    "\n",
    "vad_idsay_dataframe"
   ],
   "id": "ad93deb33c909ec0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_vad_idsay_scores = vad_idsay_dataframe[\"Scores_Given\"].apply(pd.Series).sum()\n",
    "\n",
    "all_vad_idsay_scores = all_vad_idsay_scores.reindex([1, 2, 3, 4, 5])\n",
    "\n",
    "all_vad_idsay_scores\n",
    "plt.bar(all_vad_idsay_scores.index, all_vad_idsay_scores.values)\n",
    "plt.title(\"VAD - Total number of scores across all questions for I'd say.\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(vad_idsay_dataframe[\"Mean_Score\"], bins=10, range=(0, 5), edgecolor='black')\n",
    "plt.title(\"VAD - Histogram of Mean Scores for I'd say\")\n",
    "plt.show()"
   ],
   "id": "fdea669ba390dc7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vad_elsesay_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Scores_Given\": [],\n",
    "    \"Mean_Score\": [],\n",
    "}\n",
    "\n",
    "vad_elsesay_col_names = [col for col in vad_data.columns if 'VAD_ElseSay' in col]\n",
    "\n",
    "for q_no, col in enumerate(vad_elsesay_col_names):\n",
    "    vad_data_elsesay = vad_data[vad_data[col].notna()]\n",
    "    current_data = vad_data_elsesay[col].tolist()\n",
    "\n",
    "    response_counts = vad_data_elsesay[col].value_counts().to_dict()\n",
    "\n",
    "    vad_elsesay_data[\"Question_Num\"].append(q_no+1)\n",
    "    vad_elsesay_data[\"Num_Answers\"].append(len(vad_data_elsesay))\n",
    "    vad_elsesay_data[\"Scores_Given\"].append(response_counts)\n",
    "    if len(vad_data_elsesay) == 0: vad_elsesay_data[\"Mean_Score\"].append(0)\n",
    "    else: vad_elsesay_data[\"Mean_Score\"].append(sum([score*count for score, count in response_counts.items()])/len(vad_data_elsesay))\n",
    "\n",
    "vad_elsesay_dataframe = pd.DataFrame(vad_elsesay_data)\n",
    "\n",
    "print(\n",
    "    \"VAD - Scores for How much I'd hear someone else say this:\"\n",
    "    \"\\n\\tAverage Number of Responses per question:\", vad_elsesay_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMean Score:\", vad_elsesay_dataframe[\"Mean_Score\"].mean(),\n",
    "    \"\\n\\tStandard Dev of Mean Scores:\", vad_elsesay_dataframe[\"Mean_Score\"].std(),\n",
    "    \"\\n\\tVariance of Mean Score:\", vad_elsesay_dataframe[\"Mean_Score\"].var()\n",
    ")\n",
    "\n",
    "vad_elsesay_dataframe"
   ],
   "id": "cd4531dea509c1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_vad_elsesay_scores = vad_elsesay_dataframe[\"Scores_Given\"].apply(pd.Series).sum()\n",
    "\n",
    "all_vad_elsesay_scores = all_vad_elsesay_scores.reindex([1, 2, 3, 4, 5])\n",
    "\n",
    "all_vad_elsesay_scores\n",
    "plt.bar(all_vad_elsesay_scores.index, all_vad_elsesay_scores.values)\n",
    "plt.title(\"VAD - Total number of scores across all questions for Someone else'd say.\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(vad_elsesay_dataframe[\"Mean_Score\"], bins=10, range=(0, 5), edgecolor='black')\n",
    "plt.title(\"VAD - Histogram of Mean Scores for Someone else'd say\")\n",
    "plt.show()"
   ],
   "id": "7656f8d206dfdccb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## VAD Numeric",
   "id": "f3a3c7660ea4dd9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vadnum_questions_col_names = [f\"VADNum_Question_{n}\" for n in range(1, 37)]\n",
    "vadnum_questions_data = vadnum_data[vadnum_questions_col_names]\n",
    "\n",
    "vadnum_data_cols = []\n",
    "\n",
    "for q_no, col in enumerate(vadnum_questions_col_names):\n",
    "    vadnum_data_qn = vadnum_questions_data[vadnum_questions_data[col].notna()]\n",
    "    current_data = vadnum_data_qn[col].tolist()\n",
    "    while len(current_data) < 15:\n",
    "        current_data.append(np.nan)\n",
    "    vadnum_data_cols.append(current_data)\n",
    "    # print(f\"{len(vad_data_qn)} people answered VAD Question {q_no}. Each answer was picked this many times:\", vad_data_qn[col].value_counts().to_dict())\n",
    "\n",
    "alpha = krippendorff.alpha(reliability_data=vadnum_data_cols, level_of_measurement='nominal')\n",
    "print(\"Kripp's alpha\", alpha)"
   ],
   "id": "def599afb0b68e6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Per User Likert\n",
   "id": "96ec565d4100132c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vadnum_users_list = vadnum_data[\"Q07\"].tolist()",
   "id": "1d27319234f6d0ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vadnum_convey_scores, vadnum_idsay_scores, vadnum_elsesay_scores = [], [], []\n",
    "\n",
    "for user in vadnum_users_list:\n",
    "    curr_user_data = vadnum_data[vadnum_data['Q07'] == user]    \n",
    "    curr_user_data_dropped = curr_user_data.dropna(axis=1, how='any', inplace=True)    \n",
    "    convey_answers = [curr_user_data[col].tolist()[0] for col in curr_user_data.columns if 'Convey' in col]\n",
    "    idsay_answers = [curr_user_data[col].tolist()[0] for col in curr_user_data.columns if 'IdSay' in col]\n",
    "    elsesay_answers = [curr_user_data[col].tolist()[0] for col in curr_user_data.columns if 'ElseSay' in col] \n",
    "    convey_avg, idsay_avg, elsesay_avg = np.mean(convey_answers), np.mean(idsay_answers), np.mean(elsesay_answers)\n",
    "    vadnum_convey_scores.append(convey_avg), vadnum_idsay_scores.append(idsay_avg), vadnum_elsesay_scores.append(elsesay_avg)\n",
    "    print(f\"For User: {user},\", \"Convey Score:\", convey_avg, \"I'd Say Score\", idsay_avg, \"Someone Else Say Score\", elsesay_avg)"
   ],
   "id": "4dd55cea771df734",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**For each choice question:**\n",
    "* Number of answers - 8 on average\n",
    "* Which answers and how many times - See table\n",
    "* User agreement (krippendorf's something?) - Full agreement on none. Average shannon entropy of 0.70 across the whole set of questions.\n",
    "* Did answers converge more for later questions vs earlier questions? - They did not. Users did not become more confident as they answered more questions."
   ],
   "id": "2f43d5ed8a46d73e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vadnum_q_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Answers_Picked\": [],\n",
    "    \"Answers_Normalized\": [],\n",
    "    \"Shannon_Entropy\": []\n",
    "}\n",
    "\n",
    "for q_no, col in enumerate(vadnum_questions_col_names):\n",
    "    vadnum_data_qn = vadnum_questions_data[vadnum_questions_data[col].notna()]\n",
    "    current_data = vadnum_data_qn[col].tolist()\n",
    "\n",
    "    response_counts = vadnum_data_qn[col].value_counts().to_dict()\n",
    "    proportions = {option: response_counts.get(option, 0) / sum(response_counts.values()) for option in [1, 2, 3, 4]}\n",
    "\n",
    "    vadnum_q_data[\"Question_Num\"].append(q_no + 1)\n",
    "    vadnum_q_data[\"Num_Answers\"].append(len(vadnum_data_qn))\n",
    "    vadnum_q_data[\"Answers_Picked\"].append(response_counts)\n",
    "    vadnum_q_data[\"Answers_Normalized\"].append(proportions)\n",
    "    vadnum_q_data[\"Shannon_Entropy\"].append(entropy(list(proportions.values()), base=4))\n",
    "\n",
    "vadnum_d_dataframe = pd.DataFrame(vadnum_q_data)\n",
    "\n",
    "print(\n",
    "    \"Number of Answers for VAD Numeric Choice Questions:\"\n",
    "    \"\\n\\tAverage:\", vadnum_d_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMax:\", vadnum_d_dataframe[\"Num_Answers\"].max(),\n",
    "    \"\\n\\tMin:\", vadnum_d_dataframe[\"Num_Answers\"].min(),\n",
    "    \"\\n\\tMean Entropy:\", vadnum_d_dataframe[\"Shannon_Entropy\"].mean()\n",
    ")\n",
    "\n",
    "vadnum_d_dataframe"
   ],
   "id": "358bc265435d1d57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_vadnum_questions_answers = vadnum_d_dataframe[\"Answers_Picked\"].apply(pd.Series).sum()\n",
    "\n",
    "all_vadnum_questions_answers.index = all_vadnum_questions_answers.index.astype(str).map(response_representation_map)\n",
    "all_vadnum_questions_answers = all_vadnum_questions_answers.reindex(list(response_representation_map.values()))\n",
    "\n",
    "plt.bar(all_vadnum_questions_answers.index, all_vadnum_questions_answers.values)\n",
    "plt.title(\"VAD Numeric - Overall Answers selected\")\n",
    "plt.show()"
   ],
   "id": "80b59c510422f694",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vadnum_convey_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Scores_Given\": [],\n",
    "    \"Mean_Score\": [],\n",
    "}\n",
    "\n",
    "vadnum_convey_col_names = [col for col in words_data.columns if 'VADNum_Convey' in col]\n",
    "\n",
    "for q_no, col in enumerate(vadnum_convey_col_names):\n",
    "    vadnum_data_convey = vadnum_data[vadnum_data[col].notna()]\n",
    "    current_data = vadnum_data_convey[col].tolist()\n",
    "\n",
    "    response_counts = vadnum_data_convey[col].value_counts().to_dict()\n",
    "\n",
    "    vadnum_convey_data[\"Question_Num\"].append(q_no + 1)\n",
    "    vadnum_convey_data[\"Num_Answers\"].append(len(vadnum_data_convey))\n",
    "    vadnum_convey_data[\"Scores_Given\"].append(response_counts)\n",
    "    if len(vadnum_data_convey) == 0:\n",
    "        vadnum_convey_data[\"Mean_Score\"].append(0)\n",
    "    else:\n",
    "        vadnum_convey_data[\"Mean_Score\"].append(\n",
    "            sum([score * count for score, count in response_counts.items()]) / len(vadnum_data_convey))\n",
    "\n",
    "vadnum_convey_dataframe = pd.DataFrame(vadnum_convey_data)\n",
    "\n",
    "print(\n",
    "    \"VAD Numeric - Scores for How well this Conveys:\"\n",
    "    \"\\n\\tAverage Number of Responses per question:\", vadnum_convey_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMean Score:\", vadnum_convey_dataframe[\"Mean_Score\"].mean(),\n",
    "    \"\\n\\tStandard Dev of Mean Scores:\", vadnum_convey_dataframe[\"Mean_Score\"].std(),\n",
    "    \"\\n\\tVariance of Mean Score:\", vadnum_convey_dataframe[\"Mean_Score\"].var()\n",
    ")\n",
    "\n",
    "# vadnum_convey_dataframe"
   ],
   "id": "8f7cba8856676d14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_vadnum_convey_scores = vadnum_convey_dataframe[\"Scores_Given\"].apply(pd.Series).sum()\n",
    "\n",
    "all_vadnum_convey_scores = all_vadnum_convey_scores.reindex([1, 2, 3, 4, 5])\n",
    "\n",
    "all_vadnum_convey_scores\n",
    "plt.bar(all_vadnum_convey_scores.index, all_vadnum_convey_scores.values)\n",
    "plt.title(\"VAD Numeric - Total number of scores across all questions for Convey\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(vadnum_convey_dataframe[\"Mean_Score\"], bins=10, range=(0, 5), edgecolor='black')\n",
    "plt.title('VAD Numeric - Histogram of Mean Scores for Convey')\n",
    "plt.show()"
   ],
   "id": "4fe0b3b5dfc8f245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vadnum_idsay_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Scores_Given\": [],\n",
    "    \"Mean_Score\": [],\n",
    "}\n",
    "\n",
    "vadnum_idsay_col_names = [col for col in vad_data.columns if 'VADNum_IdSay' in col]\n",
    "\n",
    "for q_no, col in enumerate(vadnum_idsay_col_names):\n",
    "    vadnum_data_idsay = vadnum_data[vadnum_data[col].notna()]\n",
    "    current_data = vadnum_data_idsay[col].tolist()\n",
    "\n",
    "    response_counts = vadnum_data_idsay[col].value_counts().to_dict()\n",
    "\n",
    "    vadnum_idsay_data[\"Question_Num\"].append(q_no + 1)\n",
    "    vadnum_idsay_data[\"Num_Answers\"].append(len(vadnum_data_idsay))\n",
    "    vadnum_idsay_data[\"Scores_Given\"].append(response_counts)\n",
    "    if len(vadnum_data_idsay) == 0:\n",
    "        vadnum_idsay_data[\"Mean_Score\"].append(0)\n",
    "    else:\n",
    "        vadnum_idsay_data[\"Mean_Score\"].append(\n",
    "            sum([score * count for score, count in response_counts.items()]) / len(vadnum_data_idsay))\n",
    "\n",
    "vadnum_idsay_dataframe = pd.DataFrame(vadnum_idsay_data)\n",
    "\n",
    "print(\n",
    "    \"VAD Numeric - Scores for How much I'd say this:\"\n",
    "    \"\\n\\tAverage Number of Responses per question:\", vadnum_idsay_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMean Score:\", vadnum_idsay_dataframe[\"Mean_Score\"].mean(),\n",
    "    \"\\n\\tStandard Dev of Mean Scores:\", vadnum_idsay_dataframe[\"Mean_Score\"].std(),\n",
    "    \"\\n\\tVariance of Mean Score:\", vadnum_idsay_dataframe[\"Mean_Score\"].var()\n",
    ")\n",
    "\n",
    "vadnum_idsay_dataframe"
   ],
   "id": "241090581e301c48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_vadnum_idsay_scores = vadnum_idsay_dataframe[\"Scores_Given\"].apply(pd.Series).sum()\n",
    "\n",
    "all_vadnum_idsay_scores = all_vadnum_idsay_scores.reindex([1, 2, 3, 4, 5])\n",
    "\n",
    "all_vadnum_idsay_scores\n",
    "plt.bar(all_vadnum_idsay_scores.index, all_vadnum_idsay_scores.values)\n",
    "plt.title(\"VAD Numeric - Total number of scores across all questions for I'd say.\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(vadnum_idsay_dataframe[\"Mean_Score\"], bins=10, range=(0, 5), edgecolor='black')\n",
    "plt.title(\"VAD Numeric - Histogram of Mean Scores for I'd say\")\n",
    "plt.show()"
   ],
   "id": "2e7dc3c0dfb11ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vadnum_elsesay_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Scores_Given\": [],\n",
    "    \"Mean_Score\": [],\n",
    "}\n",
    "\n",
    "vadnum_elsesay_col_names = [col for col in vadnum_data.columns if 'VADNum_ElseSay' in col]\n",
    "\n",
    "for q_no, col in enumerate(vadnum_elsesay_col_names):\n",
    "    vadnum_data_elsesay = vadnum_data[vadnum_data[col].notna()]\n",
    "    current_data = vadnum_data_elsesay[col].tolist()\n",
    "\n",
    "    response_counts = vadnum_data_elsesay[col].value_counts().to_dict()\n",
    "\n",
    "    vadnum_elsesay_data[\"Question_Num\"].append(q_no + 1)\n",
    "    vadnum_elsesay_data[\"Num_Answers\"].append(len(vadnum_data_elsesay))\n",
    "    vadnum_elsesay_data[\"Scores_Given\"].append(response_counts)\n",
    "    if len(vadnum_data_elsesay) == 0:\n",
    "        vadnum_elsesay_data[\"Mean_Score\"].append(0)\n",
    "    else:\n",
    "        vadnum_elsesay_data[\"Mean_Score\"].append(\n",
    "            sum([score * count for score, count in response_counts.items()]) / len(vadnum_data_elsesay))\n",
    "\n",
    "vadnum_elsesay_dataframe = pd.DataFrame(vadnum_elsesay_data)\n",
    "\n",
    "print(\n",
    "    \"VAD Num - Scores for How much I'd hear someone else say this:\"\n",
    "    \"\\n\\tAverage Number of Responses per question:\", vadnum_elsesay_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMean Score:\", vadnum_elsesay_dataframe[\"Mean_Score\"].mean(),\n",
    "    \"\\n\\tStandard Dev of Mean Scores:\", vadnum_elsesay_dataframe[\"Mean_Score\"].std(),\n",
    "    \"\\n\\tVariance of Mean Score:\", vadnum_elsesay_dataframe[\"Mean_Score\"].var()\n",
    ")\n",
    "\n",
    "vadnum_elsesay_dataframe"
   ],
   "id": "cb5c7ee9e3819377",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_vadnum_elsesay_scores = vadnum_elsesay_dataframe[\"Scores_Given\"].apply(pd.Series).sum()\n",
    "\n",
    "all_vadnum_elsesay_scores = all_vadnum_elsesay_scores.reindex([1, 2, 3, 4, 5])\n",
    "\n",
    "all_vadnum_elsesay_scores\n",
    "plt.bar(all_vadnum_elsesay_scores.index, all_vadnum_elsesay_scores.values)\n",
    "plt.title(\"VAD Numeric - Total number of scores across all questions for Someone else'd say.\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(vadnum_elsesay_dataframe[\"Mean_Score\"], bins=10, range=(0, 5), edgecolor='black')\n",
    "plt.title(\"VAD Numeric - Histogram of Mean Scores for Someone else'd say\")\n",
    "plt.show()"
   ],
   "id": "a0fcd4324e3a6eb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Emojis",
   "id": "56362dd0c2194a25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emojis_questions_col_names = [f\"Emojis_Question_{n}\" for n in range(1, 37)]\n",
    "emojis_questions_data = emojis_data[emojis_questions_col_names]\n",
    "\n",
    "emojis_data_cols = []\n",
    "\n",
    "for q_no, col in enumerate(emojis_questions_col_names):\n",
    "    emojis_data_qn = emojis_questions_data[emojis_questions_data[col].notna()]\n",
    "    current_data = emojis_data_qn[col].tolist()\n",
    "    while len(current_data) < 9:\n",
    "        current_data.append(\"\")\n",
    "    emojis_data_cols.append(current_data)\n",
    "    # print(f\"{len(vad_data_qn)} people answered VAD Question {q_no}. Each answer was picked this many times:\", vad_data_qn[col].value_counts().to_dict())\n",
    "\n",
    "alpha = krippendorff.alpha(reliability_data=emojis_data_cols, level_of_measurement='nominal')\n",
    "print(\"Kripp's alpha\", alpha)"
   ],
   "id": "c436634b3271418b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "dd2acc34db724c38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Per User Likert",
   "id": "e072c2143600b015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "emojis_users_list = emojis_data[\"Q07\"].tolist()",
   "id": "ffa94b18e6d9d066",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emojis_convey_scores, emojis_idsay_scores, emojis_elsesay_scores = [], [], []\n",
    "\n",
    "for user in emojis_users_list:\n",
    "    curr_user_data = emojis_data[emojis_data['Q07'] == user]    \n",
    "    curr_user_data_dropped = curr_user_data.dropna(axis=1, how='any', inplace=True)    \n",
    "    convey_answers = [curr_user_data[col].tolist()[0] for col in curr_user_data.columns if 'Convey' in col]\n",
    "    idsay_answers = [curr_user_data[col].tolist()[0] for col in curr_user_data.columns if 'IdSay' in col]\n",
    "    elsesay_answers = [curr_user_data[col].tolist()[0] for col in curr_user_data.columns if 'ElseSay' in col] \n",
    "    convey_avg, idsay_avg, elsesay_avg = np.mean(convey_answers), np.mean(idsay_answers), np.mean(elsesay_answers)\n",
    "    emojis_convey_scores.append(convey_avg), emojis_idsay_scores.append(idsay_avg), emojis_elsesay_scores.append(elsesay_avg)\n",
    "    print(f\"For User: {user},\", \"Convey Score:\", convey_avg, \"I'd Say Score\", idsay_avg, \"Someone Else Say Score\", elsesay_avg)"
   ],
   "id": "db98ba3fc23e57a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**For each choice question:**\n",
    "* Number of answers - 8 on average\n",
    "* Which answers and how many times - See table\n",
    "* User agreement (krippendorf's something?) - Full agreement on 11. Average shannon entropy of 0.67 across the whole set of questions.\n",
    "* Did answers converge more for later questions vs earlier questions? - They did not. Users did not become more confident as they answered more questions."
   ],
   "id": "2429875ec04f967e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emojis_q_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Answers_Picked\": [],\n",
    "    \"Answers_Normalized\": [],\n",
    "    \"Shannon_Entropy\": []\n",
    "}\n",
    "\n",
    "for q_no, col in enumerate(emojis_questions_col_names):\n",
    "    emojis_data_qn = emojis_questions_data[emojis_questions_data[col].notna()]\n",
    "    current_data = emojis_data_qn[col].tolist()\n",
    "\n",
    "    response_counts = emojis_data_qn[col].value_counts().to_dict()\n",
    "    proportions = {option: response_counts.get(option, 0)/sum(response_counts.values()) for option in [1, 2, 3, 4]}\n",
    "\n",
    "    emojis_q_data[\"Question_Num\"].append(q_no+1)\n",
    "    emojis_q_data[\"Num_Answers\"].append(len(emojis_data_qn))\n",
    "    emojis_q_data[\"Answers_Picked\"].append(response_counts)\n",
    "    emojis_q_data[\"Answers_Normalized\"].append(proportions)\n",
    "    emojis_q_data[\"Shannon_Entropy\"].append(entropy(list(proportions.values()), base=4))\n",
    "\n",
    "emojis_d_dataframe = pd.DataFrame(emojis_q_data)\n",
    "\n",
    "print(\n",
    "    \"Emojis - Number of Answers for Choice Questions:\"\n",
    "    \"\\n\\tAverage:\", emojis_d_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMax:\", emojis_d_dataframe[\"Num_Answers\"].max(),\n",
    "    \"\\n\\tMin:\", emojis_d_dataframe[\"Num_Answers\"].min(),\n",
    "    \"\\n\\tMean Entropy:\", emojis_d_dataframe[\"Shannon_Entropy\"].mean()\n",
    ")\n",
    "\n",
    "emojis_d_dataframe"
   ],
   "id": "aebc57f59a98db75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_emojis_questions_answers = emojis_d_dataframe[\"Answers_Picked\"].apply(pd.Series).sum()\n",
    "\n",
    "all_emojis_questions_answers.index = all_emojis_questions_answers.index.astype(str).map(response_representation_map)\n",
    "all_emojis_questions_answers = all_emojis_questions_answers.reindex(list(response_representation_map.values()))\n",
    "\n",
    "plt.bar(all_emojis_questions_answers.index, all_emojis_questions_answers.values)\n",
    "plt.title(\"Emojis - Overall Answers selected\")\n",
    "plt.show()"
   ],
   "id": "8bea5b410fd6669",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emojis_convey_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Scores_Given\": [],\n",
    "    \"Mean_Score\": [],\n",
    "}\n",
    "\n",
    "emojis_convey_col_names = [col for col in emojis_data.columns if 'Emojis_Convey' in col]\n",
    "\n",
    "for q_no, col in enumerate(emojis_convey_col_names):\n",
    "    emojis_data_convey = emojis_data[emojis_data[col].notna()]\n",
    "    current_data = emojis_data_convey[col].tolist()\n",
    "\n",
    "    response_counts = emojis_data_convey[col].value_counts().to_dict()\n",
    "\n",
    "    emojis_convey_data[\"Question_Num\"].append(q_no + 1)\n",
    "    emojis_convey_data[\"Num_Answers\"].append(len(emojis_data_convey))\n",
    "    emojis_convey_data[\"Scores_Given\"].append(response_counts)\n",
    "    if len(emojis_data_convey) == 0:\n",
    "        emojis_convey_data[\"Mean_Score\"].append(0)\n",
    "    else:\n",
    "        emojis_convey_data[\"Mean_Score\"].append(\n",
    "            sum([score * count for score, count in response_counts.items()]) / len(emojis_data_convey))\n",
    "\n",
    "emojis_convey_dataframe = pd.DataFrame(emojis_convey_data)\n",
    "\n",
    "print(\n",
    "    \"Emojis - Scores for How well this Conveys:\"\n",
    "    \"\\n\\tAverage Number of Responses per question:\", emojis_convey_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMean Score:\", emojis_convey_dataframe[\"Mean_Score\"].mean(),\n",
    "    \"\\n\\tStandard Dev of Mean Scores:\", emojis_convey_dataframe[\"Mean_Score\"].std(),\n",
    "    \"\\n\\tVariance of Mean Score:\", emojis_convey_dataframe[\"Mean_Score\"].var()\n",
    ")"
   ],
   "id": "7054afe7d031663d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_emojis_convey_scores = emojis_convey_dataframe[\"Scores_Given\"].apply(pd.Series).sum()\n",
    "\n",
    "all_emojis_convey_scores = all_emojis_convey_scores.reindex([1, 2, 3, 4, 5])\n",
    "\n",
    "all_emojis_convey_scores\n",
    "plt.bar(all_emojis_convey_scores.index, all_emojis_convey_scores.values)\n",
    "plt.title(\"Emojis - Total number of scores across all questions for Convey\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(emojis_convey_dataframe[\"Mean_Score\"], bins=10, range=(0, 5), edgecolor='black')\n",
    "plt.title('Emojis - Histogram of Mean Scores for Convey')\n",
    "plt.show()"
   ],
   "id": "ef926700b1210d9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emojis_idsay_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Scores_Given\": [],\n",
    "    \"Mean_Score\": [],\n",
    "}\n",
    "\n",
    "emojis_idsay_col_names = [col for col in emojis_data.columns if 'Emojis_IdSay' in col]\n",
    "\n",
    "for q_no, col in enumerate(emojis_idsay_col_names):\n",
    "    emojis_data_idsay = emojis_data[emojis_data[col].notna()]\n",
    "    current_data = emojis_data_idsay[col].tolist()\n",
    "\n",
    "    response_counts = emojis_data_idsay[col].value_counts().to_dict()\n",
    "\n",
    "    emojis_idsay_data[\"Question_Num\"].append(q_no + 1)\n",
    "    emojis_idsay_data[\"Num_Answers\"].append(len(emojis_data_idsay))\n",
    "    emojis_idsay_data[\"Scores_Given\"].append(response_counts)\n",
    "    if len(emojis_data_idsay) == 0:\n",
    "        emojis_idsay_data[\"Mean_Score\"].append(0)\n",
    "    else:\n",
    "        emojis_idsay_data[\"Mean_Score\"].append(\n",
    "            sum([score * count for score, count in response_counts.items()]) / len(emojis_data_idsay))\n",
    "\n",
    "emojis_idsay_dataframe = pd.DataFrame(emojis_idsay_data)\n",
    "\n",
    "print(\n",
    "    \"Emojis - Scores for How much I'd say this:\"\n",
    "    \"\\n\\tAverage Number of Responses per question:\", emojis_idsay_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMean Score:\", emojis_idsay_dataframe[\"Mean_Score\"].mean(),\n",
    "    \"\\n\\tStandard Dev of Mean Scores:\", emojis_idsay_dataframe[\"Mean_Score\"].std(),\n",
    "    \"\\n\\tVariance of Mean Score:\", emojis_idsay_dataframe[\"Mean_Score\"].var()\n",
    ")\n",
    "\n",
    "# emojis_idsay_dataframe"
   ],
   "id": "75eb7cd100142d74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_emojis_idsay_scores = emojis_idsay_dataframe[\"Scores_Given\"].apply(pd.Series).sum()\n",
    "\n",
    "all_emojis_idsay_scores = all_emojis_idsay_scores.reindex([1, 2, 3, 4, 5])\n",
    "\n",
    "all_emojis_idsay_scores\n",
    "plt.bar(all_emojis_idsay_scores.index, all_emojis_idsay_scores.values)\n",
    "plt.title(\"VAD Numeric - Total number of scores across all questions for I'd say.\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(emojis_idsay_dataframe[\"Mean_Score\"], bins=10, range=(0, 5), edgecolor='black')\n",
    "plt.title(\"Emojis - Histogram of Mean Scores for I'd say\")\n",
    "plt.show()"
   ],
   "id": "7b7349fc6e379c84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emojis_elsesay_data = {\n",
    "    \"Question_Num\": [],\n",
    "    \"Num_Answers\": [],\n",
    "    \"Scores_Given\": [],\n",
    "    \"Mean_Score\": [],\n",
    "}\n",
    "\n",
    "emojis_elsesay_col_names = [col for col in emojis_data.columns if 'Emojis_ElseSay' in col]\n",
    "\n",
    "for q_no, col in enumerate(emojis_elsesay_col_names):\n",
    "    emojis_data_elsesay = emojis_data[emojis_data[col].notna()]\n",
    "    current_data = emojis_data_elsesay[col].tolist()\n",
    "\n",
    "    response_counts = emojis_data_elsesay[col].value_counts().to_dict()\n",
    "\n",
    "    emojis_elsesay_data[\"Question_Num\"].append(q_no + 1)\n",
    "    emojis_elsesay_data[\"Num_Answers\"].append(len(emojis_data_elsesay))\n",
    "    emojis_elsesay_data[\"Scores_Given\"].append(response_counts)\n",
    "    if len(emojis_data_elsesay) == 0:\n",
    "        emojis_elsesay_data[\"Mean_Score\"].append(0)\n",
    "    else:\n",
    "        emojis_elsesay_data[\"Mean_Score\"].append(\n",
    "            sum([score * count for score, count in response_counts.items()]) / len(emojis_data_elsesay))\n",
    "\n",
    "emojis_elsesay_dataframe = pd.DataFrame(emojis_elsesay_data)\n",
    "\n",
    "print(\n",
    "    \"Emojis - Scores for How much I'd hear someone else say this:\"\n",
    "    \"\\n\\tAverage Number of Responses per question:\", emojis_elsesay_dataframe[\"Num_Answers\"].mean(),\n",
    "    \"\\n\\tMean Score:\", emojis_elsesay_dataframe[\"Mean_Score\"].mean(),\n",
    "    \"\\n\\tStandard Dev of Mean Scores:\", emojis_elsesay_dataframe[\"Mean_Score\"].std(),\n",
    "    \"\\n\\tVariance of Mean Score:\", emojis_elsesay_dataframe[\"Mean_Score\"].var()\n",
    ")\n",
    "\n",
    "# emojis_elsesay_dataframe"
   ],
   "id": "7863e2d126645bde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_emojis_elsesay_scores = emojis_elsesay_dataframe[\"Scores_Given\"].apply(pd.Series).sum()\n",
    "\n",
    "all_emojis_elsesay_scores = all_emojis_elsesay_scores.reindex([1, 2, 3, 4, 5])\n",
    "\n",
    "all_emojis_elsesay_scores\n",
    "plt.bar(all_emojis_elsesay_scores.index, all_emojis_elsesay_scores.values)\n",
    "plt.title(\"VAD Numeric - Total number of scores across all questions for Someone else'd say.\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(emojis_elsesay_dataframe[\"Mean_Score\"], bins=10, range=(0, 5), edgecolor='black')\n",
    "plt.title(\"Emojis - Histogram of Mean Scores for Someone else'd say\")\n",
    "plt.show()"
   ],
   "id": "e6615b2007cb98af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Per Emotion",
   "id": "65a155c936081467"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "list_of_emotions = list([em[0] for em in list(emotions_map.values())])[:18]\n",
    "columns_per_emo = {}\n",
    "per_emo_data = {}\n",
    "per_emo_stats = {}\n",
    "\n",
    "per_emo_dataset = dataset.drop(columns=[\"VAD_Train_1\", \"VAD_Train_2\", \"VAD_Train_3\", \"37_Q402_1\", \"37_Q402_2\", \"37_Q402_3\"])[:-1]\n",
    "\n",
    "for nm, em in enumerate(list_of_emotions, start=1):\n",
    "    columns_per_emo[em] = [col for col in per_emo_dataset.columns if col.endswith(f\"_{nm*2-1}\") or col.endswith(f\"_{nm*2}\")]\n",
    "    # print(\"Current Emotion:\", em)\n",
    "\n",
    "    curr_emo_data = {}\n",
    "    curr_emo_stats = {}\n",
    "    questions_cols = [col for col in columns_per_emo[em] if 'Question' in col]\n",
    "    convey_cols = [col for col in columns_per_emo[em] if 'Convey' in col]\n",
    "    idsay_cols = [col for col in columns_per_emo[em] if 'IdSay' in col]\n",
    "    elsesay_cols = [col for col in columns_per_emo[em] if 'ElseSay' in col]\n",
    "\n",
    "    curr_emo_data[\"response_counts\"] = per_emo_dataset[questions_cols].stack().value_counts().sort_index().to_dict()\n",
    "    curr_emo_data[\"convey_scores\"] = per_emo_dataset[convey_cols].stack().value_counts().sort_index().to_dict()\n",
    "    curr_emo_data[\"idsay_scores\"] = per_emo_dataset[idsay_cols].stack().value_counts().sort_index().to_dict()\n",
    "    curr_emo_data[\"elsesay_scores\"] = per_emo_dataset[elsesay_cols].stack().value_counts().sort_index().to_dict()\n",
    "\n",
    "    curr_emo_stats[\"shannon_entropy\"] = entropy([val/sum(curr_emo_data[\"response_counts\"].values()) for val in curr_emo_data[\"response_counts\"].values()], base=4)\n",
    "    curr_emo_stats[\"mean_convey_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "    curr_emo_stats[\"mean_idsay_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "    curr_emo_stats[\"mean_elsesay_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "\n",
    "    per_emo_data[em] = curr_emo_data\n",
    "    per_emo_stats[em] = curr_emo_stats\n",
    "\n"
   ],
   "id": "8e545079ac524611",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for label, ques_types in per_emo_data.items():\n",
    "    for type_of_question, responses in ques_types.items():\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(responses.keys(), responses.values(), color='skyblue')\n",
    "        plt.xlabel(\"Score\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(f\"{label} - {type_of_question}\")\n",
    "        plt.xticks(list(responses.keys()))  # Ensure correct tick labels\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.show()"
   ],
   "id": "405933438b7e434d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = list(per_emo_stats.keys())\n",
    "\n",
    "colors = cm.Spectral(np.linspace(0, 1, len(labels)))\n",
    "\n",
    "# Loop over the attributes and create a graph for each\n",
    "for key in per_emo_stats['Grateful'].keys():\n",
    "    # Extract values for the current attribute\n",
    "    values = [per_emo_stats[label][key] for label in labels]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(labels, values, color=colors)\n",
    "    plt.xlabel('Labels')\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(key.replace('_', ' ').title())\n",
    "    plt.title(f'{key.replace(\"_\", \" \").title()} for Each Emotion')\n",
    "    plt.show()"
   ],
   "id": "58a983be4e9a3c10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Per-User Tests",
   "id": "e8d7a36591ba8649"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_users = dataset[\"Q07\"].tolist()[:-1]\n",
    "practice_correct_rate = []\n",
    "questions_answers = []\n",
    "question_emotions = []\n",
    "convey_answers = []\n",
    "idsay_answers = []\n",
    "elsesay_answers = []\n",
    "\n",
    "ctct = 0\n",
    "for user in all_users:\n",
    "    filtered_row = (dataset[dataset[\"Q07\"] == user]).iloc[0]\n",
    "    repr_type = filtered_row[\"representation_type\"]\n",
    "    filtered_row = filtered_row.dropna()\n",
    "\n",
    "    if repr_type == \"1\":\n",
    "        practice_correct = True if filtered_row[\"Words_Practice\"] == 2 else False\n",
    "        practice_correct_rate.append(1.0)\n",
    "        segments = ['Question', 'Convey', 'IdSay', 'ElseSay']\n",
    "\n",
    "        filtered_columns = [col for col in filtered_row.index if any(sub in col for sub in segments)]\n",
    "        result = {key: {filtered_row[key]: 1} for key in filtered_columns}\n",
    "\n",
    "        print(result)\n",
    "        question_dict = [list(val.keys())[0] for key, val in result.items() if 'Question' in key]\n",
    "        # convey_dict = {val for key, val in result.items() if 'Convey' in key}\n",
    "        # idsay_dict = {val for key, val in result.items() if 'IdSay' in key}\n",
    "        # elsesay_dict = {val for key, val in result.items() if 'ElseSay' in key}\n",
    "\n",
    "        print(question_dict)\n",
    "\n",
    "        # questions_answers.append(list(question_dict.values()))\n",
    "        # convey_answers.append(list(convey_dict.keys()))\n",
    "        # idsay_answers.append(list(idsay_dict.keys()))\n",
    "        # elsesay_answers.append(list(elsesay_dict.keys()))\n",
    "\n",
    "    ctct += 5\n",
    "\n",
    "    if ctct == 5: break\n",
    "\n",
    "# [practice_correct,\n",
    "# question_dict,\n",
    "# convey_answers,\n",
    "# idsay_answers,\n",
    "# elsesay_answers]\n",
    "# convey_dict\n",
    "# idsay_dict\n",
    "# elsesay_dict"
   ],
   "id": "5b78bba5bfcfa874",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Statistical Tests",
   "id": "97745274a19277e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# anova = f_oneway(convey_scores, idsay_scores, elsesay_scores)\n",
    "# scores are avgs per user\n",
    "# if p-val < 0.05, do posthoc test (pairways), Tukey's HSD\n",
    "anova_convey = f_oneway(words_convey_scores, vad_convey_scores, vadnum_convey_scores, emojis_convey_scores)\n",
    "anova_idsay = f_oneway(words_idsay_scores, vad_idsay_scores, vadnum_idsay_scores, emojis_idsay_scores)\n",
    "anova_elsesay = f_oneway(words_elsesay_scores, vad_elsesay_scores, vadnum_elsesay_scores, emojis_elsesay_scores)\n",
    "\n",
    "print(\"How well does this convey---\", anova_convey)\n",
    "print(\"How well does this I'd say---\", anova_idsay)\n",
    "print(\"How well does this someone-else---\", anova_elsesay)"
   ],
   "id": "247907b3974e81a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pw_thsd_convey = tukey_hsd(words_convey_scores, vad_convey_scores, vadnum_convey_scores, emojis_convey_scores)\n",
    "pw_thsd_idsay = tukey_hsd(words_idsay_scores, vad_idsay_scores, vadnum_idsay_scores, emojis_idsay_scores)\n",
    "\n",
    "print(\"Pairwise test for Convey---\", pw_thsd_convey)\n",
    "print(\"Pairwise test for I'd say---\", pw_thsd_idsay)"
   ],
   "id": "a8e3abb3038a952e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generating Charts",
   "id": "97713f24b8112287"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.rcParams.update({'font.size': 14})  # Set default font size",
   "id": "19fbcfb34272f0d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "49d5db63d922f57e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_words_questions_answers\n",
    "all_vad_questions_answers\n",
    "all_vadnum_questions_answers\n",
    "all_emojis_questions_answers\n",
    "\n",
    "df_q_a = pd.concat([all_words_questions_answers, all_vad_questions_answers, all_vadnum_questions_answers, all_emojis_questions_answers], axis=1)\n",
    "df_q_a = df_q_a.T\n",
    "df_q_a = df_q_a.rename(index={0: \"Words\", 1: \"Lexical VAD\", 2: \"Numeric VAD\", 3: \"Emojis\"})\n",
    "df_q_a = df_q_a.rename(columns={\"VAD\": \"Lexical VAD\", \"Numeric_VAD\": \"Numeric VAD\"})\n",
    "\n",
    "df_q_a_norm = df_q_a.div(df_q_a.sum(axis=1), axis=0)\n",
    "\n",
    "colors = {\"Words\": plt.cm.Set2(1), \"Lexical VAD\": plt.cm.Set2(2), \"Numeric VAD\": plt.cm.Set2(3), \"Emojis\": plt.cm.Set2(4)}\n",
    "\n",
    "ax = df_q_a_norm.plot(kind=\"bar\", figsize=(10, 6), width=0.8, color=[colors[label] for label in df_q_a.columns], edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "plt.title(\"GPT-4\", fontsize=20, y=0.895, bbox=dict(edgecolor=\"black\", facecolor=\"white\", boxstyle=\"round, pad=0.3\", alpha=0.15))\n",
    "plt.xlabel(\"Emotion Expressed to Participant in\")\n",
    "plt.ylabel(\"Answers Selected, normalized\")\n",
    "# plt.title(\"Comparison of Different Series\")\n",
    "plt.legend(title=\"Sentence Generated using\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(0, 0.65)\n",
    "plt.savefig(\"figures/GPT4_EmoAlign_Summary.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "id": "bd72a1d1d63d42f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_q_a",
   "id": "d2d70f7d0a3c9b09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_normalized = df_q_a.div(df_q_a.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot the normalized stacked bar chart\n",
    "ax = df_normalized.plot(kind='bar', stacked=False, figsize=(8, 5), colormap='viridis')\n",
    "\n",
    "# Labeling\n",
    "plt.xlabel(\"Categories (X-Axis)\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"Normalized Stacked Bar Chart\")\n",
    "plt.legend(title=\"Features\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ],
   "id": "ea18d6d5365b2a4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "colors = [plt.cm.Set2(1), plt.cm.Set2(2), plt.cm.Set2(3), plt.cm.Set2(4)]\n",
    "\n",
    "plt.figure(figsize=(10.1, 6))\n",
    "plt.hist([w_convey_data[\"Mean_Score\"], vad_convey_data[\"Mean_Score\"], vadnum_convey_data[\"Mean_Score\"], emojis_convey_data[\"Mean_Score\"]], bins=10,\n",
    "         color=colors, label=[\"Words\", \"VAD\", \"VAD Numeric\", \"Emojis\"], edgecolor=\"black\", linewidth=0.5)\n",
    "plt.xlabel(\"Mean Scores for 'How Well does this Convey the Emotion'\")\n",
    "plt.ylabel(\"Number of people giving this Score\")\n",
    "plt.xticks(np.arange(0, 5.5, 0.5))\n",
    "plt.legend(title=\"Emotion Expressed \\nto Participant in\")\n",
    "plt.show()"
   ],
   "id": "33b763fbe8e03139",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "colors = [plt.cm.Set2(1), plt.cm.Set2(2), plt.cm.Set2(3), plt.cm.Set2(4)]\n",
    "\n",
    "plt.figure(figsize=(10.1, 6))\n",
    "plt.hist([w_idsay_data[\"Mean_Score\"], vad_idsay_data[\"Mean_Score\"], vadnum_idsay_data[\"Mean_Score\"], emojis_idsay_data[\"Mean_Score\"]], bins=10,\n",
    "         color=colors, label=[\"Words\", \"VAD\", \"VAD Numeric\", \"Emojis\"], edgecolor=\"black\", linewidth=0.5)\n",
    "plt.xlabel(\"Mean Scores for 'How Well does this Convey the Emotion'\")\n",
    "plt.ylabel(\"Number of people giving this Score\")\n",
    "plt.xticks(np.arange(0, 5.5, 0.5))\n",
    "plt.legend(title=\"Emotion Expressed \\nto Participant in\")\n",
    "plt.show()"
   ],
   "id": "4396252eedac7fa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "colors = [plt.cm.Set2(1), plt.cm.Set2(2), plt.cm.Set2(3), plt.cm.Set2(4)]\n",
    "\n",
    "plt.figure(figsize=(10.1, 6))\n",
    "plt.hist([w_elsesay_data[\"Mean_Score\"], vad_elsesay_data[\"Mean_Score\"], vadnum_elsesay_data[\"Mean_Score\"], emojis_elsesay_data[\"Mean_Score\"]], bins=10,\n",
    "         color=colors, label=[\"Words\", \"VAD\", \"VAD Numeric\", \"Emojis\"], edgecolor=\"black\", linewidth=0.5)\n",
    "plt.xlabel(\"Mean Scores for 'How Well does this Convey the Emotion'\")\n",
    "plt.ylabel(\"Number of people giving this Score\")\n",
    "plt.xticks(np.arange(0, 5.5, 0.5))\n",
    "plt.legend(title=\"Emotion Expressed \\nto Participant in\")\n",
    "plt.show()"
   ],
   "id": "8a3b1a13cec56034",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "colors = [plt.cm.Set2(1), plt.cm.Set2(2), plt.cm.Set2(3), plt.cm.Set2(4)]\n",
    "\n",
    "xls = [\"Words\", \"VAD\", \"VAD Numeric\", \"Emojis\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(\n",
    "    height=[np.mean(w_convey_data[\"Mean_Score\"]),\n",
    "    np.mean(vad_convey_data[\"Mean_Score\"]),\n",
    "    np.mean(vadnum_convey_data[\"Mean_Score\"]),\n",
    "    np.mean(emojis_convey_data[\"Mean_Score\"])],\n",
    "         color=colors, x=[\"Words\", \"Lexical VAD\", \"Numeric VAD\", \"Emojis\"], edgecolor=\"black\", linewidth=0.5, width=0.5)\n",
    "\n",
    "# for i, value in enumerate([np.mean(w_convey_data[\"Mean_Score\"]),\n",
    "#     np.mean(vad_convey_data[\"Mean_Score\"]),\n",
    "#     np.mean(vadnum_convey_data[\"Mean_Score\"]),\n",
    "#     np.mean(emojis_convey_data[\"Mean_Score\"])]):\n",
    "#     plt.text(xls[i], value - 0.2, str(round(value, 2)), ha='center', va='top', fontsize=16, color='black')\n",
    "\n",
    "plt.xlabel(\"Representations for 'Convey'\")\n",
    "plt.ylabel(\"Mean Scores\")\n",
    "plt.ylim(0.0, 5.0)\n",
    "plt.title(\"GPT-4\", fontsize=20, y=1.03, bbox=dict(edgecolor=\"black\", facecolor=\"white\", boxstyle=\"round, pad=0.3\", alpha=0.15))\n",
    "plt.savefig(\"figures/WordsConveySummary_gpt4.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(\n",
    "    height=[np.mean(w_idsay_data[\"Mean_Score\"]),\n",
    "    np.mean(vad_idsay_data[\"Mean_Score\"]),\n",
    "    np.mean(vadnum_idsay_data[\"Mean_Score\"]),\n",
    "    np.mean(emojis_idsay_data[\"Mean_Score\"])],\n",
    "         color=colors, x=[\"Words\", \"Lexical VAD\", \"Numeric VAD\", \"Emojis\"], edgecolor=\"black\", linewidth=0.5, width=0.5)\n",
    "plt.xlabel(\"Representations for 'I'd Say'\")\n",
    "plt.ylabel(\"Mean Scores\")\n",
    "plt.ylim(0.0, 5.0)\n",
    "plt.savefig(\"figures/WordsIdSaySummary_gpt4.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "colors = [plt.cm.Set2(1), plt.cm.Set2(2), plt.cm.Set2(3), plt.cm.Set2(4)]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(\n",
    "    height=[np.mean(w_convey_data[\"Mean_Score\"]),\n",
    "    np.mean(vad_convey_data[\"Mean_Score\"]),\n",
    "    np.mean(vadnum_convey_data[\"Mean_Score\"]),\n",
    "    np.mean(emojis_convey_data[\"Mean_Score\"])],\n",
    "         color=colors, x=[\"Words\", \"Lexical VAD\", \"Numeric VAD\", \"Emojis\"], edgecolor=\"black\", linewidth=0.5, width=0.5)\n",
    "plt.xlabel(\"Representation for 'Someone Else Say'\")\n",
    "plt.ylabel(\"Mean Scores\")\n",
    "plt.ylim(0.0, 5.0)\n",
    "plt.savefig(\"figures/WordsElseSaySummary_gpt4.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "id": "3e28f0a16c1ad241",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "per_emo_words_data = {}\n",
    "per_emo_words_stats = {}\n",
    "reprs_to_exclude = [item for item in list(per_emo_dataset.columns) if any(s in item for s in [\"VAD\", \"VADNum\", \"Emojis\"])]\n",
    "per_emo_words_dataset = dataset.drop(columns=reprs_to_exclude)[:-1]\n",
    "\n",
    "# per_emo_words_dataset[questions_cols]\n",
    "\n",
    "for nm, em in enumerate(list_of_emotions, start=1):\n",
    "    columns_per_emo[em] = [col for col in per_emo_words_dataset.columns if col.endswith(f\"_{nm*2-1}\") or col.endswith(f\"_{nm*2}\")]\n",
    "    # print(\"Current Emotion:\", em)\n",
    "\n",
    "    curr_emo_data = {}\n",
    "    curr_emo_stats = {}\n",
    "    questions_cols = [col for col in columns_per_emo[em] if 'Question' in col]\n",
    "    convey_cols = [col for col in columns_per_emo[em] if 'Convey' in col]\n",
    "    idsay_cols = [col for col in columns_per_emo[em] if 'IdSay' in col]\n",
    "    elsesay_cols = [col for col in columns_per_emo[em] if 'ElseSay' in col]\n",
    "\n",
    "    curr_emo_data[\"response_counts\"] = per_emo_words_dataset[questions_cols].stack().value_counts().sort_index().to_dict()\n",
    "\n",
    "    curr_emo_data[\"convey_scores\"] = per_emo_words_dataset[convey_cols].stack().value_counts().sort_index().to_dict()\n",
    "    curr_emo_data[\"idsay_scores\"] = per_emo_words_dataset[idsay_cols].stack().value_counts().sort_index().to_dict()\n",
    "    curr_emo_data[\"elsesay_scores\"] = per_emo_words_dataset[elsesay_cols].stack().value_counts().sort_index().to_dict()\n",
    "\n",
    "    curr_emo_stats[\"shannon_entropy\"] = entropy([val/sum(curr_emo_data[\"response_counts\"].values()) for val in curr_emo_data[\"response_counts\"].values()], base=4)\n",
    "    curr_emo_stats[\"mean_convey_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "    curr_emo_stats[\"mean_idsay_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "    curr_emo_stats[\"mean_elsesay_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "\n",
    "    per_emo_words_data[em] = curr_emo_data\n",
    "    per_emo_words_stats[em] = curr_emo_stats\n",
    "\n",
    "per_emo_words_stats"
   ],
   "id": "2c775510888b5553",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "per_emo_vad_data = {}\n",
    "per_emo_vad_stats = {}\n",
    "reprs_to_exclude = [item for item in list(per_emo_dataset.columns) if any(s in item for s in [\"Words\", \"VADNum\", \"Emojis\"])]\n",
    "per_emo_vad_dataset = dataset.drop(columns=reprs_to_exclude)[:-1]\n",
    "\n",
    "for nm, em in enumerate(list_of_emotions, start=1):\n",
    "    columns_per_emo[em] = [col for col in per_emo_vad_dataset.columns if col.endswith(f\"_{nm*2-1}\") or col.endswith(f\"_{nm*2}\")]\n",
    "    # print(\"Current Emotion:\", em)\n",
    "\n",
    "    curr_emo_data = {}\n",
    "    curr_emo_stats = {}\n",
    "    questions_cols = [col for col in columns_per_emo[em] if 'Question' in col]\n",
    "    convey_cols = [col for col in columns_per_emo[em] if 'Convey' in col]\n",
    "    idsay_cols = [col for col in columns_per_emo[em] if 'IdSay' in col]\n",
    "    elsesay_cols = [col for col in columns_per_emo[em] if 'ElseSay' in col]\n",
    "\n",
    "    curr_emo_data[\"response_counts\"] = per_emo_vad_dataset[questions_cols].stack().value_counts().sort_index().to_dict()\n",
    "\n",
    "    curr_emo_data[\"convey_scores\"] = per_emo_vad_dataset[convey_cols].stack().value_counts().sort_index().to_dict()\n",
    "    curr_emo_data[\"idsay_scores\"] = per_emo_vad_dataset[idsay_cols].stack().value_counts().sort_index().to_dict()\n",
    "    curr_emo_data[\"elsesay_scores\"] = per_emo_vad_dataset[elsesay_cols].stack().value_counts().sort_index().to_dict()\n",
    "\n",
    "    curr_emo_stats[\"shannon_entropy\"] = entropy([val/sum(curr_emo_data[\"response_counts\"].values()) for val in curr_emo_data[\"response_counts\"].values()], base=4)\n",
    "    curr_emo_stats[\"mean_convey_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "    curr_emo_stats[\"mean_idsay_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "    curr_emo_stats[\"mean_elsesay_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "\n",
    "    per_emo_vad_data[em] = curr_emo_data\n",
    "    per_emo_vad_stats[em] = curr_emo_stats\n",
    "\n",
    "per_emo_vad_stats"
   ],
   "id": "f5dc19913377fb85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "per_emo_vadnum_data = {}\n",
    "per_emo_vadnum_stats = {}\n",
    "reprs_to_exclude = [item for item in list(per_emo_dataset.columns) if any(s in item for s in [\"Words\", \"VAD_\", \"Emojis\"])]\n",
    "per_emo_vadnum_dataset = dataset.drop(columns=reprs_to_exclude)[:-1]\n",
    "\n",
    "\n",
    "\n",
    "for nm, em in enumerate(list_of_emotions, start=1):\n",
    "    columns_per_emo[em] = [col for col in per_emo_vadnum_dataset.columns if col.endswith(f\"_{nm*2-1}\") or col.endswith(f\"_{nm*2}\")]\n",
    "    # print(\"Current Emotion:\", em)\n",
    "\n",
    "    curr_emo_data = {}\n",
    "    curr_emo_stats = {}\n",
    "    questions_cols = [col for col in columns_per_emo[em] if 'Question' in col]\n",
    "    convey_cols = [col for col in columns_per_emo[em] if 'Convey' in col]\n",
    "    idsay_cols = [col for col in columns_per_emo[em] if 'IdSay' in col]\n",
    "    elsesay_cols = [col for col in columns_per_emo[em] if 'ElseSay' in col]\n",
    "\n",
    "    curr_emo_data[\"response_counts\"] = per_emo_vadnum_dataset[questions_cols].stack().value_counts().sort_index().to_dict()\n",
    "\n",
    "    curr_emo_data[\"convey_scores\"] = per_emo_vadnum_dataset[convey_cols].stack().value_counts().sort_index().to_dict()\n",
    "    curr_emo_data[\"idsay_scores\"] = per_emo_vadnum_dataset[idsay_cols].stack().value_counts().sort_index().to_dict()\n",
    "    curr_emo_data[\"elsesay_scores\"] = per_emo_vadnum_dataset[elsesay_cols].stack().value_counts().sort_index().to_dict()\n",
    "\n",
    "    curr_emo_stats[\"shannon_entropy\"] = entropy([val/sum(curr_emo_data[\"response_counts\"].values()) for val in curr_emo_data[\"response_counts\"].values()], base=4)\n",
    "    curr_emo_stats[\"mean_convey_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "    curr_emo_stats[\"mean_idsay_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "    curr_emo_stats[\"mean_elsesay_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "\n",
    "    per_emo_vadnum_data[em] = curr_emo_data\n",
    "    per_emo_vadnum_stats[em] = curr_emo_stats\n",
    "\n",
    "per_emo_vadnum_stats"
   ],
   "id": "2b19ad48ecb28bdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "per_emo_emojis_data = {}\n",
    "per_emo_emojis_stats = {}\n",
    "reprs_to_exclude = [item for item in list(per_emo_dataset.columns) if any(s in item for s in [\"Words\", \"VAD\", \"VADNum\"])]\n",
    "per_emo_emojis_dataset = dataset.drop(columns=reprs_to_exclude)[:-1]\n",
    "\n",
    "for nm, em in enumerate(list_of_emotions, start=1):\n",
    "    columns_per_emo[em] = [col for col in per_emo_emojis_dataset.columns if col.endswith(f\"_{nm*2-1}\") or col.endswith(f\"_{nm*2}\")]\n",
    "    # print(\"Current Emotion:\", em)\n",
    "\n",
    "    curr_emo_data = {}\n",
    "    curr_emo_stats = {}\n",
    "    questions_cols = [col for col in columns_per_emo[em] if 'Question' in col]\n",
    "    convey_cols = [col for col in columns_per_emo[em] if 'Convey' in col]\n",
    "    idsay_cols = [col for col in columns_per_emo[em] if 'IdSay' in col]\n",
    "    elsesay_cols = [col for col in columns_per_emo[em] if 'ElseSay' in col]\n",
    "\n",
    "    curr_emo_data[\"response_counts\"] = per_emo_emojis_dataset[questions_cols].stack().value_counts().sort_index().to_dict()\n",
    "\n",
    "    curr_emo_data[\"convey_scores\"] = per_emo_emojis_dataset[convey_cols].stack().value_counts().sort_index().to_dict()\n",
    "    curr_emo_data[\"idsay_scores\"] = per_emo_emojis_dataset[idsay_cols].stack().value_counts().sort_index().to_dict()\n",
    "    curr_emo_data[\"elsesay_scores\"] = per_emo_emojis_dataset[elsesay_cols].stack().value_counts().sort_index().to_dict()\n",
    "\n",
    "    curr_emo_stats[\"shannon_entropy\"] = entropy([val/sum(curr_emo_data[\"response_counts\"].values()) for val in curr_emo_data[\"response_counts\"].values()], base=4)\n",
    "    curr_emo_stats[\"mean_convey_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "    curr_emo_stats[\"mean_idsay_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "    curr_emo_stats[\"mean_elsesay_score\"] = sum(key*val for key, val in curr_emo_data[\"convey_scores\"].items())/sum(curr_emo_data[\"convey_scores\"].values())\n",
    "\n",
    "    per_emo_emojis_data[em] = curr_emo_data\n",
    "    per_emo_emojis_stats[em] = curr_emo_stats\n",
    "\n",
    "per_emo_emojis_stats"
   ],
   "id": "8fb4ffd4f034f98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emotions_list = list(per_emo_words_stats.keys())\n",
    "\n",
    "shannon_entropy_values = []\n",
    "for data in [per_emo_words_stats, per_emo_vad_stats, per_emo_vadnum_stats, per_emo_emojis_stats]:\n",
    "    shannon_entropy_values.append([data[emotion]['shannon_entropy'] for emotion in emotions_list])\n",
    "\n",
    "shannon_entropy_data = np.array(shannon_entropy_values)\n",
    "\n",
    "shannon_entropy_df = pd.DataFrame(shannon_entropy_values, columns=emotions_list, index=['Words', 'VAD', 'VADNum', 'Emojis'])\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "cax = plt.imshow(shannon_entropy_df, cmap=\"plasma_r\", alpha=0.7, interpolation='nearest')\n",
    "\n",
    "\n",
    "plt.xticks(ticks=np.arange(18), labels=emotions_list)\n",
    "plt.yticks(ticks=np.arange(4), labels=['Words', 'Lex VAD', 'Num VAD', 'Emojis'])\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(18):\n",
    "        value = shannon_entropy_data[i, j]\n",
    "        plt.text(j, i, f'{value:.2f}', ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "# plt.title(\"Heatmap of Shannon Entropy Values per Emotion and Representation\")\n",
    "# plt.xlabel(\"Emotion\", fontsize=14)\n",
    "plt.xticks(rotation=25, ha=\"center\")\n",
    "plt.ylabel(\"Representation\", fontsize=16)\n",
    "plt.savefig(\"figures/Shannon_Entropy_Per_Emotion_Representation_GPT-4.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "id": "e26df41d578c59e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean_convey_values = []\n",
    "for data in [per_emo_words_stats, per_emo_vad_stats, per_emo_vadnum_stats, per_emo_emojis_stats]:\n",
    "    mean_convey_values.append([data[emotion]['mean_convey_score'] for emotion in emotions_list])\n",
    "\n",
    "mean_convey_data = np.array(mean_convey_values)\n",
    "\n",
    "mean_convey_df = pd.DataFrame(mean_convey_values, columns=emotions_list, index=['Words', 'VAD', 'VADNum', 'Emojis'])\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "cax = plt.imshow(mean_convey_df, cmap=\"plasma\", alpha=0.7, interpolation='nearest')\n",
    "\n",
    "\n",
    "plt.xticks(ticks=np.arange(18), labels=emotions_list)\n",
    "plt.yticks(ticks=np.arange(4), labels=['Words', 'Lex VAD', 'Num VAD', 'Emojis'])\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(18):\n",
    "        value = mean_convey_data[i, j]\n",
    "        plt.text(j, i, f'{value:.2f}', ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "# plt.title(\"Heatmap of Shannon Entropy Values per Emotion and Representation\")\n",
    "# plt.xlabel(\"Emotion\", fontsize=14)\n",
    "plt.xticks(rotation=25, ha=\"center\")\n",
    "plt.ylabel(\"Representation\", fontsize=16)\n",
    "plt.savefig(\"figures/Mean_Convey_Per_Emotion_Representation_GPT-4.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "id": "c82b0b232713fb68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_q_a_vals = pd.concat([all_words_questions_answers, all_vad_questions_answers, all_vadnum_questions_answers, all_emojis_questions_answers], axis=1)\n",
    "df_q_a_vals = df_q_a_vals.T\n",
    "df_q_a_vals = df_q_a_vals.rename(index={0: \"Words\", 1: \"VAD\", 2: \"Numeric_VAD\", 3: \"Emojis\"})\n",
    "\n",
    "df_q_a_vals\n",
    "diagonal_values = np.diag(df_q_a_vals)\n",
    "row_sums = df_q_a.sum(axis=1)\n",
    "\n",
    "result = (diagonal_values / row_sums) * 100\n",
    "print(result)\n"
   ],
   "id": "93bb3f9439d03a33",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
